---
title: "Review and Cleaning of Casco Bay OA Data"
output:
  html_document:
    df_print: paged
---

# Load Libraries
```{r}
library(tidyverse)
library(readxl)
library(CBEPgraphics)
```

```{r}
load_cbep_fonts()
```


# Load Data
## Establish Folder References
```{r}
sibfldnm <- 'Original Data'
parent   <- dirname(getwd())
sibling  <- file.path(parent,sibfldnm)

```


# Load Data
From similar excel spreadsheets.

This code originally generated a lot of warnings because the second row of each
spreadsheet contains the units used.  Since R is looking for numeric data, it
gets confused. There is a solution -- to read just the data header from the
first file, then read data from all the files skipping the first two rows -- but
it is simpler to just use "suppressWarnings" to silence the (now expected)
warnings.  However this suppresses ALL warnings, including new ones that you may
need to see.  If something is not working, remove the call to suppressWarnings
and see if the resulting warnings help identify the problem.
```{r}
the_data <- list()
for (yr in c(2015, 2016, 2017, 2018)) {
  fn <- paste0('CascoBay_lvl3_',  yr ,'.xls')
  fpath <- file.path(sibling,fn)
  suppressWarnings(df <- read_excel(fpath, col_types = 'numeric',
                   sheet = 'Data')[-1,])  %>%  # dropping garbage second line
  select(- contains('_min')) %>%
  select(- contains('_mean')) %>%
  select(- contains('_max')) %>%
  select(- contains('_std'))
  n <- names(df)
  n <- sub('_median', '', n)
  names(df) <- n
  the_data <- append(the_data, list(df))  #  add dataframe to a list of dataframes. 
}
```

# Combine Data
```{r}
the_data <- bind_rows(the_data) %>%
  select(-yyyymmdd, -Matlab_datenum, -FET_TEMP_CON) %>%
  mutate(datetime = ISOdatetime(yyyy, mm, dd, hh,0,0, 'America/New_York')) %>%
  mutate(doy = as.numeric(strftime(datetime, format = "%j")))
```

In downstream analysis steps, I uncovered duplicate dates and times that lack
data, specifically from January of 2016. I filter them out here.

## Demonstrate the Problem
```{r}
the_data %>% select(datetime) %>% group_by(datetime) %>% summarize(n = n()) %>% filter(n>1) %>% arrange(datetime)
```

## Drop any records that have no data
```{r}
the_data <-  the_data %>% filter(
   ! (
     is.na(SBE37_TEMP) &
     is.na(SBE37_CON) &
     is.na(SBE37_SALINITY) &
     is.na(SAMICO2_TEMP) &
     is.na(SAMI_CO2) &
     is.na(Optode_O2) &
     is.na(FET_PHINT) &
     is.na(FET_PHEXT) &  
     is.na(`omega-a`) &
     is.na(`omega-c`) &
     is.na(TA_calc) &    
     is.na(DIC_calc)
   )
)

```

# Filtering out Bad PH Data
Larry Harris pointed out that certain pH observations look impossible.  Many low
pH values look suspect, especially in 2017.  In addition, any pH measurements
associated with calculated alkalinity below about 1000 should be considered
highly suspect, since low alkalinity is unlikely at high salinity.

We explore these problems graphically.

## Graph of pH data over time
```{r}
plt <- ggplot(the_data, aes(datetime, FET_PHINT)) +
  geom_point(aes(color = TA_calc), alpha = 0.05) +
  xlab('Date') +
  ylab('pH') +
  theme_cbep()
plt
```
I note low pH excursions in 2015, perhaps 2016, and most of 2017. But note that
the fist "spike" of low values in 2015 lack alkalinity calculations, presumably
because other required data was not available.

## Graph of pH versus Alkalinity
(Note that since calculated alkalinity is only possible when all other data is
available, this graph omits a lot of observations, including some low pH
values.)
```{r}
plt <- ggplot(the_data, aes(FET_PHINT, TA_calc, color = factor(yyyy))) +
  geom_point(alpha = 0.1, size = 0.5) +
  xlab('pH') +
  ylab('Alkalinity') +
  theme_cbep() +
   guides(colour = guide_legend(override.aes = list(alpha = 1, size = 1)))
plt
```
The most obvious problematic data (with very low alkalinity AND very low pH)
are from 2015 and 2017.

## Graph of Alkalinity vs. Salinity
```{r}
plt <- ggplot(the_data, aes(SBE37_SALINITY, TA_calc, color = factor(yyyy))) +
  geom_point(alpha = 0.2, size = 1)  +
  xlab('Salinity') +
  ylab('Alkalinity') +
  guides(colour = guide_legend(override.aes = list(alpha = 1, size = 4))) +
  scale_x_continuous(limits = c(22,32)) +
  theme_cbep()
plt
```
Again, what jumps out are the points with low pH in 2017 and 2015.  Although
the HIGH alkalinity observations from early in 2017 also look off-base.

If we look at a plot of pH versus temperature, the same points jump out, with
possibly some other low pH observations from 2016.
```{r}
plt <- ggplot(the_data, aes(SBE37_TEMP, FET_PHINT, color = factor(yyyy))) +
  geom_point(alpha = 0.1) +
  theme_cbep() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  xlab('Temperature (C)') +
    ylab('pH') +
  guides(colour = guide_legend(override.aes = list(alpha = 1)))
plt
```
I see no obvious way to eliminate the erroneous data.  There are "bad" data
popping up several times.  It's mostly the 2017 data, but there may also be
problematic data in 2015 and 2016.

# Removing Questionable Data
## 2017
Identify the date in 2017 when things went wonky.
```{r}
plt <- the_data %>% filter(yyyy==2017, mm == 6, dd<18, dd>10) %>%
  ggplot(aes(x=datetime, y=FET_PHINT)) + geom_line() +
  theme_cbep() +
  xlab('Date')
plt
```
The pH drop off kicks in around June 14th. or 15th.

So, things went odd around June 14th.

# 2015
```{r}
plt <- the_data %>% filter(yyyy==2015, mm %in% c(8,9,10)) %>%
  ggplot(aes(x=datetime, y=FET_PHINT)) + 
  geom_line(aes(color=(TA_calc<1200)))+
  theme_cbep() +
  xlab('Date')
plt
```

That shows a low pH period, in September around end of September 8 through Sept
14.  That one may actually have started just before the data gap, around August
17th and another around September 30 through October 11.  The exact period we
should remove from the data here is somewhat arbitrary.  Since the primary
indicator of problematic data is the low observed pH value, we risk biasing
results.  Here we back that up by tossing out data by whole days, and by
focusing on days with low calculated alkalinity.

# Remove Questionable Data
Remove questionable pH values and ALSO CO2Sys calculated values
```{r}
d1 <- ISOdatetime(2015,8, 17,0,0, 0,'America/New_York')
d2 <- ISOdatetime(2015,9,14,0,0, 0,'America/New_York')
d3 <- ISOdatetime(2015,9,30,0,0, 0,'America/New_York')
d4 <- ISOdatetime(2015,10,11,0,0, 0,'America/New_York')
d5 <- ISOdatetime(2017,6,14,0,0, 0,'America/New_York')
d6 <- ISOdatetime(2017,12,31,0,0,0,'America/New_York')
phomitflag1  <- ! (the_data$datetime < d1 | the_data$datetime > d2)
phomitflag2  <- ! (the_data$datetime < d3 | the_data$datetime > d4)
phomitflag3  <- ! (the_data$datetime < d5 | the_data$datetime > d6)
flag <- ! (phomitflag1 | phomitflag2 | phomitflag3)
rm(phomitflag1, phomitflag2, phomitflag3)
```

# Show What pH Data Will be Removed
```{r}
tt <- the_data %>%  mutate(t = flag) %>% filter(yyyy==2015)

plt <- ggplot(tt, aes(x=datetime, y=FET_PHINT)) +
  geom_point(aes(color = t),size = 0.5, alpha = 0.2) +
  theme_cbep() +
  xlab('Date') +
  guides(colour = guide_legend(override.aes = list(alpha = 1)))
plt
rm(tt)
```
We are still left with some low values, but the  really wonky sudden pH changes are
removed. 

# Remove Unwanted Data
To trim the data, we use the flags to set to NA any variables that depend on accurate
pH values, and then toss out any data that lacks remaining valid observations.
```{r}
trimmed_data <- the_data %>%
  mutate(FET_PHINT = ifelse(flag, FET_PHINT, NA)) %>%
  mutate(FET_PHEXT = ifelse(flag, FET_PHEXT, NA)) %>%
  mutate(`omega-a` = ifelse(flag, `omega-a`, NA)) %>%
  mutate(`omega-c` = ifelse(flag, `omega-c`, NA)) %>%
  mutate(TA_calc = ifelse(flag, TA_calc, NA)) %>%
  mutate(DIC_calc = ifelse(flag, DIC_calc, NA)) %>%
  filter_all(any_vars(!is.na(.)))
```

# Output Cleaned Data
```{r}
write_csv(trimmed_data,'CascoBayOAData.csv')
```
