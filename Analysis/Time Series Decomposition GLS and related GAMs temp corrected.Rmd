---
title: "Time Series Analysis of Temperature COrrected pCO~2~ Data from Casco Bay"
output: html_notebook
---

# Introduction
This R Notebook looks at preliminary models for analyzing temperature-corrected pCO~2~ data from CBEP.

Our principal goal is to determine how best to model time dependence of pCO~2~ values.  Once we settle on an approach to modelling time dependnece in GAMS, we will compare results with the results from methods piloted by EPA.

Our goal is fitting full GAM models, while the EPA method fits linear models to diurnal summaries (means) of "detrended" data, where both predictors and response variables have been detrended ansd sumamrized based on daily means.

Because our long-term interest is in comparing "what appears to be most important" for predicting carbonate chemistry at multiple  NEP, we need to explore the same model space for all NEPs.  WE probe alternate model structures for CBEP as a way to determine whether conclusins are sensitive to model specification or not.

Conceptually, the model space we are exploring includes associations between dependent variables and
*  Temperature
*  Salinity
*  Dissolved Oxygen

But things are more complicated than that suggests.  
1.  Relationships (for any or all of these three predictors) may be non-linear.  
2.  Interactions between predictors are possible (although their meaning may not be clear).  
3.  Strong temporal structure (diurnal; seasonal; possibly tidal; serial autocorrelation) will obscure or confound simple relationships  

## Proposed Models
This suggests we need to fit generalized additive models (GAMs), with autocorrelated errors.  There are many possible models that could address our needs.  We fit three classes of models.

The first pair of models  are of the following form:
1.  Adjust for time of year using a categorical variable.
2.  Adjust for time of day using a periodic smoothing function. 
3.  Fit the three quantitative predictors as linear terms.
4.  Include an autoregressive error term of order one.

$$y_{t} = \mu_{m} +\beta_{1}c_{h}(h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma  * \epsilon_{t-1} + \epsilon_{t}$$

Where:  
*  $y_{t}$ is the dependent time series, here pCO~2~.  
*  $T$, $S$, and $D$, are Temperature, Salinity, and Dissolved Oxygen.  
*  $m$ represents the month (of the year).  
*  $h$ designates the hour of the day.  
*  $\mu_{m}$ are (twelve) fitten means by month of the year.  This models seasonality.  
*  $c_{h}(h)$ is a periodic diurnal smoothing function. This models diurnal patterns.  
*  $\beta_{i}$ represent (linear) regression coefficients.  
*  $\gamma$ represents the correlation with the error of the prior observation.  
*  $\epsilon_{t}$ is a sequence of "innovations" or errors.  

The difference between the first two models lies the form of the diurnal smoothing function, which is either constructed from two sine and two cosine terms or fit using a cyclic cubic spline function.

The third model fits a smoothing function for month instead of fitting a categorical adjustment to the mean for each month.

$$y_{t} = \mu + \beta_{0}c_{m}(m)+\ +\beta_{1}c_{h}(h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma \epsilon_{t-1} + \epsilon_{t}$$

Where terms have the same meaning as before and
*  $\mu$ is an overall mean.
*  $c_{m}(m)$ is a smoothing function for time of year by month.

The fourth model addresses seasonality not on a month by month basis, but on a day by day basis.

$$y_{t} = \mu + \beta_{0}c_{d}(d)+\ +\beta_{1}c_{h}(h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma \epsilon_{t-1} + \epsilon_{t}$$

where  
*  $d$ indexes the day of the year, also known as the Julian day.  
*  $c_{d}(d)$ is a seasonal smoothing function  

The final model, fits a two dimensional temporal smoothing function that in practice allows the diurnal pattern of pCO~2~ to vary by time of year. 

$$y_{t} = \mu + \beta_{1}c_{dh}(d,h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma \epsilon_{t-1} + \epsilon_{t}$$

where   
*  $c_{dh}(d,h)$ is a two dimensional smoothing function (cyclic cubic soline) based on time of day and time of year.

In practice, this final model's peridic smoothing function was fit in three parts, $C_{d}(d), C_{h}(h)), and c_{d,h}(d,h).  That allows a formal test of whether the two dimensional smoother is necessary.  It is, and with all three terms in the model, it is equivalent to fitting a single two dimensional smoother.

# Load Libraries
```{r}
library(tidyverse)
library(readxl)
library(CBEPgraphics)

library(nlme)      # includes gls function
#library(zoo)       # assists with time series calculations 
#library(lubridate) # simplifies management of date and time objects

library(mgcv)      # One of two common libraries for general additive models.
                   # Function gamm allows autocorrelation.
                   # plot allows examination of GAM fit components
#library(gamm4)     # A faster (?) version of gamm from mgcv, relying on lme4
                   # Fails in this use setting because if does not allow
                   # Correlation structures.
#library(ff)        # Handles large data frames more efficiently with memory


#library(emmeans)   # Provides tools for calculating marginal means
#library(ggeffects) # provides access functions to emmeans that return tidy tibbles and facilitate plots

#library(broom)     # Allows ready conversion to tidy tibbles


```

# Load Data 
## Establish Folder References
```{r}
sibfldnm <- 'Complete NEP Data'
parent   <- dirname(getwd())
sibling  <- file.path(parent,sibfldnm)

fn    <- 'CB_oa_data.csv'
fpath <- file.path(sibling,fn)
```
## Initiailze fonts
```{r message=FALSE}
load_cbep_fonts()
```

## Look Up Data
```{r} 
the_data <- read_csv(fpath) %>% select(-X1) %>% select(-WaterBody) %>%
  rename_at(vars(contains('_meas')), funs(str_replace(., '_meas','')))  %>%
  rename_all(tolower) %>%
  rename(co2_corr = co2_tcorr12) %>%
    mutate(yyyy = as.numeric(format(datetime, format = '%Y')),
         mm   = as.numeric(format(datetime, format = '%m')),
         dd   = as.numeric(format(datetime, format = '%d')),
         hh   = as.numeric(format(datetime, format = '%H')),
         doy  = as.numeric(format(datetime, format = '%j'))
         )
```

# Correcting Time Series
Most modelling functions in R, when applied to time series, assume you are working with a complete regular timeseries, and thus do not always correctly calculate autocorrelations, especially around dates adn times omitted from the data.

(You can, in part get arond this with in corAR1 by specifying form= ~datetime).

So our first step is to rectify our time series.  We do this in two steps.  First, we creat a "complete" data set, with NAs added as necessary to fill all time steps.  Second, to save space, we toss out any date or time too far from "real" data to affect our models.  Here we plan only to use lag 1 or perhaps lag 2 models, so we don't need dates and times more than one or two steps (hours) away from real data.  

## Check that timestamps are consistent 
For this to work, all time stamps have to be rounded to the same time boundaries, here each hour on the hour. We can check that by looking at the "minute" value form each datetime value.  
```{r}
range(as.numeric(format(the_data$datetime, format = '%M')), na.rm=TRUE)
```
Oh oh....  Some timestamps are on the half hour.  All are from late in 2018. We round them down to the next lowest full hour.  
```{r}
the_data <- the_data %>%
  mutate(datetime = as.POSIXct(paste0(yyyy, '-', mm, '-', dd, ' ', hh, ':00'),
                               format='%Y-%m-%d %H:%M'))

```

## Create a Data Frame containing all hourly time stamps    
```{r}
firstdt <- min(the_data$datetime, na.rm = TRUE)
lastdt  <- max(the_data$datetime, na.rm = TRUE)
alldt <- tibble(datetime =seq(firstdt, lastdt, "hour"))
range(alldt$datetime)
```
## Use left_join to merge data into that dataframe
And regenerate the timestamps.
```{r}
full_data <- left_join(alldt, the_data, by='datetime') %>%
  select(-yyyy, -dd, -hh, -doy) %>%
  mutate(yyyy = as.numeric(format(datetime, format = '%Y')),
         mm   = as.numeric(format(datetime, format = '%m')),
         dd   = as.numeric(format(datetime, format = '%d')),
         hh   = as.numeric(format(datetime, format = '%H')),
         doy  = as.numeric(format(datetime, format = '%j'))
         )
rm(alldt, firstdt, lastdt, the_data)
```

## Now delete unnecessary "blank" lines
We want to identify lines to drop.  Any datetime line that is more than two hours away from "real data" can be dropped.

However, we need to recognize that for model comparison, each model must be fitted to the same data. For legitimate model comparison, all models must also be built with ML, not REML methods. Once an optimal model is selected, it can be refit with REML.

```{r}
full_data <- full_data %>%
  # First we identify lines that have complete data EITHER for pH or pCO2
  # We need all three predictors and either pH or pCO2.
  mutate(has_data = ! (is.na(temp) | is.na(sal) | is.na(do)) & 
                    ! (is.na(co2)   & is.na(ph))) %>%
  # Then create a shifted version of that data
  # I specify dplyr because dplyr and base lag functions work differently
  mutate(next_has_data = dplyr::lead(has_data),  
         after_has_data = dplyr::lead(has_data,2)) %>%
  filter(has_data | next_has_data | after_has_data) %>%
  select (-has_data, -next_has_data, -after_has_data) %>%
  mutate(Month = factor(mm, levels = 1:12, labels = month.abb) ) %>%
  mutate(Hour = factor (hh, levels = 0:23))
``` 

# Data Analysis
Here I fit  models using GLS or GAMM functions.  These only fit linear terms relating the predictors to the OA parameters.  This is an over simplification, but a useful starting point for understanding data structure, and mimics the statistical methods done by the folks at EPA.

These models  sometimes either failed to run, or took a very long time.   The problem appears to be a shortage of working memory, as it sometimes offered a warning about not being able to allocate enough memory.  This problem was severe on my work computer, and less so on my home computer, which has many fewer services running in the background.  

```{r}
full_data <- full_data %>% select(-season, -dd, -Hour, -yyyy, -co2)
gc()   # Trigger garbage collection

```

# An intitial GLS to test methods
This took about 15 minutes to run on my home computer.
```{r}
the_gls <- gls(co2_corr ~  Month +
                 sin(((2*pi)/24)*hh) + cos(((2*pi)/24)*hh) +
                 sin(((4*pi)/24)*hh) + cos(((4*pi)/24)*hh) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
summary(the_gls)
```

```{r}
cc <- coef(the_gls)[13:16]
hh = 1:24
vv =  cc[1] * sin(((2 * pi)/24) * hh) + 
      cc[2] * cos(((2 * pi)/24) * hh) +
      cc[3] * sin(((4 * pi)/24) * hh) +
      cc[4] * cos(((4 * pi)/24) * hh)
plot(hh,vv, type = 'l')
```

```{r}
saveRDS(the_gls, 'GLS.corr.rds')
rm(the_gls)
gc()
```


# An initial GAM, with only linear terms on the predictors
This took a bit over half an hour on my home computer.  Longer if R had been open for some time.  perhaps I should run gc() again.
```{r}
the_gam <- gamm(co2_corr ~  Month +
                 s(hh, bs="cc") +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```

```{r}
summary(the_gam$lme)
```


```{r}
plot(the_gam$gam)
```
That, of course, looks a lot like the model we fit using gls.`
  
```{r}
acf(resid(the_gam$lme), main = 'GLS Residuals', na.action= na.pass)
pacf(resid(the_gam$lme), main = 'GLS Residuals', na.action= na.pass)
```
So, there is still some periodic structure in the errors with a roughly 24 hour period.  Note the high partial autocorrelation at period 1.  That is expected, as the model does not REMOVE the ar1 term, it only accounts for it.


```{r}
saveRDS(the_gam, 'GAM1.corr.rds')
```

# Lower Degree of Freedom Models of Seasonality
Lets look at lower degree of freedom models for seasonality
```{r}
rm(the_gam)
gc()
the_gam_mm <- gamm(co2_corr ~  s(mm, bs='cc', k=5) +
                 s(hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```

```{r}
plot(the_gam_mm$gam)
```



```{r}
saveRDS(the_gam_mm, 'GAM1.mm.corr.rds')
```


```{r}
rm(the_gam_mm)
gc()
the_gam_doy <- gamm(co2_corr ~  s(doy, bs="cc", k=5) +
                 s(hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```


```{r}
plot(the_gam_doy$gam)
```

```{r}
saveRDS(the_gam_doy, 'GAM1.doy.1.corr.rds')

```


```{r}
rm(the_gam_doy)
gc()
the_gam_doy_2 <- gamm(co2_corr ~  ti(doy, bs="cc", k=5) + ti(hh, bs="cc", k=5) + ti(doy,hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')

```



```{r}
plot(the_gam_doy_2$gam)
```

```{r}
saveRDS(the_gam_doy_2, 'GAM1.doy.2.corr.rds')

```


```{r}
anova(the_gam_doy_2$gam)
```

