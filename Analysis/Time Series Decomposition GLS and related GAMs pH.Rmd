---
title: "Time Series Analysis of pH data"
output: html_notebook
---

# Introduction
This R Notebook looks at preliminary models for analyzing pH data from CBEP.

Our principal goal is to determine how best to model time dependence of pH values.  Once we settle on an approach to modelling time dependnece in GAMS, we will compare results with the results from methods piloted by EPA.

Because our long-term interest is in comparing "what appears to be most important" for predicting carbonate chemistry at multiple  NEP, we need to explore the same model space for all NEPs.  WE probe alternate model structures for CBEP as a way to determine whether conclusins are sensitive to model specification or not.

Conceptually, the model space we are exploring includes associations between dependent variables and
*  Temperature
*  Salinity
*  Dissolved Oxygen

But things are more complicated than that suggests.  
1.  Relationships (for any or all of these three predictors) may be non-linear.  
2.  Interactions between predictors are possible (although their meaning may not be clear).  
3.  Strong temporal structure (diurnal; seasonal; possibly tidal; serial autocorrelation) will obscure or confound simple relationships  

## Proposed Models
This suggests we need to fit generalized additive models (GAMs), with autocorrelated errors.  There are many possible models that could address our needs.  We fit three classes of models.

The first pair of models  are of the following form:
1.  Adjust for time of year using a categorical variable.
2.  Adjust for time of day using a periodic smoothing function. 
3.  Fit the three quantitative predictors as linear terms.
4.  Include an autoregressive error term of order one.

$$y_{t} = \mu_{m} +\beta_{1}c_{h}(h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma  * \epsilon_{t-1} + \epsilon_{t}$$
Where:  
*  $y_{t}$ is the dependent time series, here pH.  
*  $T$, $S$, and $D$, are Temperature, Salinity, and Dissolved Oxygen.  
*  $m$ represents the month (of the year).  
*  $h$ designates the hour of the day.  
*  $\mu_{m}$ are (twelve) fitten means by month of the year.  This models seasonality.  
*  $c_{h}(h)$ is a periodic diurnal smoothing function. This models diurnal patterns.  
*  $\beta_{i}$ represent (linear) regression coefficients.  
*  $\gamma$ represents the correlation with the error of the prior observation.  
*  $\epsilon_{t}$ is a sequence of "innovations" or errors.  

The difference between the first two models lies the form of the diurnal smoothing function, which is either constructed from two sine and two cosine terms or fit using a cyclic cubic spline function.

The third model fits a smoothing function for month instead of fitting a categorical adjustment to the mean for each month.

$$y_{t} = \mu + \beta_{0}c_{m}(m)+\ +\beta_{1}c_{h}(h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma \epsilon_{t-1} + \epsilon_{t}$$
Where terms have the same meaning as before and
*  $\mu$ is an overall mean.
*  $c_{m}(m)$ is a smoothing function for time of year by month.  Note, however, that here the assumption of a cyclic smoother for time of year may be inappropriate, since we have no pH data from December through March.  Later, I try models with both cyclic and non-cyclic fits.

The fourth model addresses seasonality not on a month by month basis, but on a day by day basis.  Again, a cyclic cubic spline smoother may be inappropriate here.

$$y_{t} = \mu + \beta_{0}c_{d}(d)+\ +\beta_{1}c_{h}(h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma \epsilon_{t-1} + \epsilon_{t}$$
where  
*  $d$ indexes the day of the year, also known as the Julian day.  
*  $c_{d}(d)$ is a seasonal smoothing function  

The final model, fits a two dimensional temporal smoothing function that in practice allows the diurnal pattern of pH to vary by time of year. 

$$y_{t} = \mu + \beta_{1}c_{dh}(d,h) + \beta_{2}T +\beta_{3}S + \beta_{4}D + \gamma \epsilon_{t-1} + \epsilon_{t}$$
where   
*  $c_{dh}(d,h)$ is a two dimensional smoothing function (cyclic cubic soline) based on time of day and time of year.

(in practice, this final model's peridic smoothing function was fit in three parts, $C_{d}(d), C_{h}(h)), and c_{d,h}(d,h).  That allows a formal test of whether the two dimensional smoother is necessary.  It is, and with all three terms in the model, it is equivalent to fitting a single two dimensional smoother.

##Proposed Modelling Strategy 
We can begin to fit simplified linear models with GLS, to explore autocorrelation of errors, and confirm that an corAR1() error structure is likely to be appropriate, We then move on to GAMs.

#  A Warning
Here, as we do not have, and do not know how to calculate, temperature adjusted pH values, we work with "raw" pH values.  Most of the code used in this Notebook should work equally wellwith the other data.

# Load Libraries
```{r}
library(tidyverse)
library(readxl)
library(CBEPgraphics)

library(nlme)      # includes gls function
library(zoo)       # assists with time series calculations 
library(lubridate) # simplifies management of date and time objects

library(mgcv)      # One of two common libraries for general additive models.
                   # Function gamm allows autocorrelation.
                   # plot allows examination of GAM fit components
#library(gamm4)     # A faster (?) version of gamm from mgcv, relying on lme4
                   # Fails in this use setting because if does not allow
                   # Correlation structures.
#library(ff)        # Handles large data frames more efficiently with memory


library(emmeans)   # Provides tools for calculating marginal means
library(ggeffects) # provides access functions to emmeans that return tidy tibbles and facilitate plots

library(broom)     # Allows ready conversion to tidy tibbles


```

# Load Data 
## Establish Folder References
```{r}
sibfldnm <- 'Complete NEP Data'
parent   <- dirname(getwd())
sibling  <- file.path(parent,sibfldnm)

fn    <- 'CB_oa_data.csv'
fpath <- file.path(sibling,fn)
```
## Initiailze fonts
```{r}
load_cbep_fonts()
```

## Look Up Data
```{r} 
the_data <- read_csv(fpath) %>% select(-X1) %>% select(-WaterBody) %>%
  rename_at(vars(contains('_meas')), funs(str_replace(., '_meas','')))  %>%
  rename_all(tolower) %>%
  rename(co2_corr = co2_tcorr12) %>%
    mutate(yyyy = as.numeric(format(datetime, format = '%Y')),
         mm   = as.numeric(format(datetime, format = '%m')),
         dd   = as.numeric(format(datetime, format = '%d')),
         hh   = as.numeric(format(datetime, format = '%H')),
         doy  = as.numeric(format(datetime, format = '%j'))
         )
```

# Correcting Time Series
Most modelling functions in R, when applied to time series, assume you are working with a complete regular timeseries, and thus do not always correctly calculate autocorrelations, especially around dates adn times omitted from the data.

(You can, in part get arond this with in corAR1 by specifying form= ~datetime).

So our first step is to rectify our time series.  We do this in two steps.  First, we creat a "complete" data set, with NAs added as necessary to fill all time steps.  Second, to save space, we toss out any date or time too far from "real" data to affect our models.  Here we plan only to use lag 1 or perhaps lag 2 models, so we don't need dates and times more than one or two steps (hours) away from real data.  

## Check that timestamps are consistent 
For this to work, all time stamps have to be rounded to the same time boundaries, here each hour on the hour. We can check that by looking at the "minute" value form each datetime value.  
```{r}
range(as.numeric(format(the_data$datetime, format = '%M')), na.rm=TRUE)
```
Oh oh....  Some timestamps are on the half hour.  All are from late in 2018. We round them down to the next lowest full hour.  
```{r}
the_data <- the_data %>%
  mutate(datetime = as.POSIXct(paste0(yyyy, '-', mm, '-', dd, ' ', hh, ':00'),
                               format='%Y-%m-%d %H:%M'))

```

## Create a Data Frame containing all hourly time stamps    
```{r}
firstdt <- min(the_data$datetime, na.rm = TRUE)
lastdt  <- max(the_data$datetime, na.rm = TRUE)
alldt <- tibble(datetime =seq(firstdt, lastdt, "hour"))
range(alldt$datetime)
```
## Use left_join to merge data into that dataframe
And regenerate the timestamps.
```{r}
full_data <- left_join(alldt, the_data, by='datetime') %>%
  select(-yyyy, -dd, -hh, -doy) %>%
  mutate(yyyy = as.numeric(format(datetime, format = '%Y')),
         mm   = as.numeric(format(datetime, format = '%m')),
         dd   = as.numeric(format(datetime, format = '%d')),
         hh   = as.numeric(format(datetime, format = '%H')),
         doy  = as.numeric(format(datetime, format = '%j'))
         )
rm(alldt, firstdt, lastdt, the_data)
```

## Now delete unnecessary "blank" lines
We want to identify lines to drop.  Any datetime line that is more than two hours away from "real data" can be dropped.

However, we need to recognize that for model comparison, each model must be fitted to the same data. For legitimate model comparison, all models must also be built with ML, not REML methods. Once an optimal model is selected, it can be refit with REML.

```{r}
full_data <- full_data %>%
  # First we identify lines that have complete data EITHER for pH or pCO2
  # We need all three predictors and either pH or pCO2.
  mutate(has_data = ! (is.na(temp) | is.na(sal) | is.na(do)) & 
                    ! (is.na(co2)   & is.na(ph))) %>%
  # Then create a shifted version of that data
  # I specify dplyr because dplyr and base lag functions work differently
  mutate(next_has_data = dplyr::lead(has_data),  
         after_has_data = dplyr::lead(has_data,2)) %>%
  filter(has_data | next_has_data | after_has_data) %>%
  select (-has_data, -next_has_data, -after_has_data) %>%
  mutate(Month = factor(mm, levels = 1:12, labels = month.abb) ) %>%
  mutate(Hour = factor (hh, levels = 0:23))
``` 
# Data Analysis

## Initial Models
Here I fit a couple of models using GLS or GAMM functions.  These only fit linear terms relating the predictors to the OA parameters.  This is an over simplification, but a useful starting point for understanding data structure.

Note here I only fit linear predictors, other than for time of day.

These models  took too much memory and either failed to run, or took a very long time.   The problem appears to be a shortage of working memory, as it sometimes offered a warning about not being able to allocate enough memory.  This problem wass severe on my work computer, and less so on my home computer, whcih has many fewer services running in the background.  

Here I fit a more efficient model that fits a periodic function for time of day.  But I STILL ran into memory problems.  Presumably, the problem is that the full data for CBEP is huge.  By default, R holds objects entirely in memory.   A lot of searching online failed to find a ready way to run gamm() using tools that swap portions of the data into and out of working memory (e.g. package ff).

If this fails to run, it should be possible to separate teh model into time series and non-time series components, as Nick's tatistician friend did.

```{r}
full_data <- full_data %>% select(-co2, -co2_corr, -season, -dd, -Hour, -yyyy)
#full_data <- as.ffdf(full_data)
gc()   # Trigger garbage collection

```

##  An intitial GLS to test methods
This took about 15 minutes to run on my home computer.
```{r}
the_gls <- gls(ph ~  Month +
                 sin(((2*pi)/24)*hh) + cos(((2*pi)/24)*hh) +
                 sin(((4*pi)/24)*hh) + cos(((4*pi)/24)*hh) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
summary(the_gls)
```

```{r}
hh = 1:24
vv =  -4.5346 * sin(((2 * pi)/24) * hh) + 
     -19.0444 * cos(((2 * pi)/24) * hh) +
      -3.1567 * sin(((4 * pi)/24) * hh) +
       0.9477 * cos(((4 * pi)/24) * hh)
plot(hh,vv, type = 'l')
```

```{r}
saveRDS(the_gls, 'GLS_ph.rds')
rm(the_gls)
gc()
```


# An Related GAM
This took a bit over half an hour on my home computer.  Longer if R had been open for some time.  perhaps I should run gc() again.
```{r}
the_gam <- gamm(ph ~  Month +
                 s(hh, bs="cc") +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```

```{r}
summary(the_gam$lme)
```


```{r}
plot(the_gam$gam)
```

```{r}
acf(resid(the_gam$lme), main = 'GLS Residuals', na.action= na.pass)
pacf(resid(the_gam$lme), main = 'GLS Residuals', na.action= na.pass)
```
Note the high partial autocorrelation at period 1. 

```{r}
saveRDS(the_gam, 'GAM1_ph.rds')
```

# Other Models of Seasonality
Lets look at lower degree of freedom models for seasonality
```{r}
rm(the_gam)
gc()
the_gam_mm <- gamm(ph ~  s(mm, bs='cc', k=5) +
                 s(hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```

```{r}
plot(the_gam_mm$gam)
```



```{r}
saveRDS(the_gam_mm, 'GAM1.mm_ph.rds')
```

```{r}
rm(the_gam_mm)
gc()
the_gam_mm_nocycle <- gamm(ph ~  s(mm, bs='cs', k=5) +
                 s(hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```

```{r}
plot(the_gam_mm_nocycle$gam)
```


```{r}
saveRDS(the_gam_mm_nocycle, 'GAM1.mm_ph_nocycle.rds')
```



```{r}
rm(the_gam_mm_nocycle)
gc()
the_gam_doy <- gamm(ph ~  s(doy, bs="cc", k=5) +
                 s(hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```


```{r}
plot(the_gam_doy$gam)
```

```{r}
saveRDS(the_gam_doy, 'GAM1.doy.1_ph.rds')

```

```{r}
rm(the_gam_mm_doy)
gc()
the_gam_doy_nocycle <- gamm(ph ~  s(doy, bs="cs", k=5) +
                 s(hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')
```


```{r}
plot(the_gam_doy_nocycle$gam)
```

```{r}
saveRDS(the_gam_doy_nocycle, 'GAM1.doy.1_ph_nocycle.rds')

```


```{r}
rm(the_gam_doy_nocycle)
gc()
the_gam_doy_2 <- gamm(ph ~  ti(doy, bs="cc", k=5) + ti(hh, bs="cc", k=5) + ti(doy,hh, bs="cc", k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')

```



```{r}
plot(the_gam_doy_2$gam)
```

```{r}
saveRDS(the_gam_doy_2, 'GAM1.doy.2_ph.rds')

```

```{r}
rm(the_gam_doy_2)
gc()
the_gam_doy_2_nocycle <- gamm(ph ~  ti(doy, bs="cs", k=5) + ti(hh, bs="cc", k=5) + ti(doy,hh, bs=c("cs","cc"), k=5) +
                 temp + sal + do,
                 correlation=corAR1(), data = full_data,
                 na.action = na.exclude, method='ML')

```



```{r}
plot(the_gam_doy_2_nocycle$gam)
```

```{r}
saveRDS(the_gam_doy_2_nocycle, 'GAM1.doy.2_ph_nocycle.rds')

```


